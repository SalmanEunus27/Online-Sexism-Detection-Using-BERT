BERT stands for Bidirectional Encoder Representations from Transformers. BERT has been designed to pre-train deep bidirectional representations from the unlabelled text by joint conditioning on both the left and right context in all layers. The pre-trained BERT can be fine-tuned with just one extra layer to create state-of-the-art models for a wide range of tasks like - question answering and language inference, without substantial task-specific architecture alterations [3]. For Task - A we are using DistilBERT which is a small fast, cheap, and light Transformer model trained by distilling BERT base.  It has 40% fewer parameters than google-bert/bert-base-uncased and runs 60% faster while preserving over 95% of BERTâ€™s performances as measured on the GLUE language understanding benchmark. In addition, we are also using a sentence transform as well along with DistilBERT.  The sentence transformer model is denoted in figures 1 & 2 as Sentence2Sentence layer maps sentences and paragraphs to a 384-dimensional dense vector space. This sentence transformer is known as all-MiniLM-L6-v2 and is often used for tasks like clustering or semantic search. We have combined both of these models to form an ensemble model.
